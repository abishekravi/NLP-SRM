{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc2e9bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0d3268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\extended_omw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\omw.zip.\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\omw-1.4.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2021.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet31.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\Abishek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7625c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code courtesy of https://nlpforhackers.io/language-models/\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Create a placeholder for model\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# Count frequency of co-occurance  \n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        model[(w1, w2)][w3] += 1\n",
    " \n",
    "# Let's transform the counts to probabilities\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16c6c059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'public': 0.05555555555555555,\n",
       " 'European': 0.05555555555555555,\n",
       " 'Bank': 0.05555555555555555,\n",
       " 'price': 0.1111111111111111,\n",
       " 'emirate': 0.05555555555555555,\n",
       " 'overseas': 0.05555555555555555,\n",
       " 'newspaper': 0.05555555555555555,\n",
       " 'company': 0.16666666666666666,\n",
       " 'Turkish': 0.05555555555555555,\n",
       " 'increase': 0.05555555555555555,\n",
       " 'options': 0.05555555555555555,\n",
       " 'Higher': 0.05555555555555555,\n",
       " 'pound': 0.05555555555555555,\n",
       " 'Italian': 0.05555555555555555,\n",
       " 'time': 0.05555555555555555}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the next word\n",
    "dict(model[\"today\",\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7994590e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yesterday': 0.004651162790697674,\n",
       " 'of': 0.3209302325581395,\n",
       " 'it': 0.05581395348837209,\n",
       " 'effect': 0.004651162790697674,\n",
       " 'cut': 0.009302325581395349,\n",
       " 'for': 0.05116279069767442,\n",
       " 'paid': 0.013953488372093023,\n",
       " 'to': 0.05581395348837209,\n",
       " 'increases': 0.013953488372093023,\n",
       " 'used': 0.004651162790697674,\n",
       " 'climate': 0.004651162790697674,\n",
       " '.': 0.023255813953488372,\n",
       " 'cuts': 0.009302325581395349,\n",
       " 'reductions': 0.004651162790697674,\n",
       " 'limit': 0.004651162790697674,\n",
       " 'now': 0.004651162790697674,\n",
       " 'moved': 0.004651162790697674,\n",
       " 'per': 0.013953488372093023,\n",
       " 'adjustments': 0.004651162790697674,\n",
       " '(': 0.009302325581395349,\n",
       " 'slumped': 0.004651162790697674,\n",
       " 'is': 0.018604651162790697,\n",
       " 'move': 0.004651162790697674,\n",
       " 'evolution': 0.004651162790697674,\n",
       " 'differentials': 0.009302325581395349,\n",
       " 'went': 0.004651162790697674,\n",
       " 'the': 0.013953488372093023,\n",
       " 'factor': 0.004651162790697674,\n",
       " 'Royal': 0.004651162790697674,\n",
       " ',': 0.018604651162790697,\n",
       " 'again': 0.004651162790697674,\n",
       " 'changes': 0.004651162790697674,\n",
       " 'holds': 0.004651162790697674,\n",
       " 'has': 0.009302325581395349,\n",
       " 'fall': 0.004651162790697674,\n",
       " '-': 0.004651162790697674,\n",
       " 'from': 0.004651162790697674,\n",
       " 'base': 0.004651162790697674,\n",
       " 'on': 0.004651162790697674,\n",
       " 'review': 0.004651162790697674,\n",
       " 'while': 0.004651162790697674,\n",
       " 'collapse': 0.004651162790697674,\n",
       " 'being': 0.004651162790697674,\n",
       " 'at': 0.023255813953488372,\n",
       " 'outlook': 0.004651162790697674,\n",
       " 'rises': 0.004651162790697674,\n",
       " 'drop': 0.004651162790697674,\n",
       " 'guaranteed': 0.004651162790697674,\n",
       " ',\"': 0.004651162790697674,\n",
       " 'stayed': 0.009302325581395349,\n",
       " 'structure': 0.004651162790697674,\n",
       " 'and': 0.004651162790697674,\n",
       " 'could': 0.004651162790697674,\n",
       " 'related': 0.004651162790697674,\n",
       " 'hike': 0.004651162790697674,\n",
       " 'we': 0.004651162790697674,\n",
       " 'adjustment': 0.023255813953488372,\n",
       " 'policy': 0.004651162790697674,\n",
       " 'was': 0.009302325581395349,\n",
       " 'revision': 0.004651162790697674,\n",
       " 'freeze': 0.009302325581395349,\n",
       " 'led': 0.004651162790697674,\n",
       " 'action': 0.004651162790697674,\n",
       " 'zone': 0.004651162790697674,\n",
       " 'slump': 0.004651162790697674,\n",
       " 'had': 0.004651162790697674,\n",
       " 'difference': 0.004651162790697674,\n",
       " 'in': 0.004651162790697674,\n",
       " 'raise': 0.004651162790697674,\n",
       " 'increase': 0.009302325581395349,\n",
       " 'will': 0.013953488372093023,\n",
       " 'support': 0.004651162790697674,\n",
       " 'gap': 0.004651162790697674,\n",
       " 'would': 0.009302325581395349,\n",
       " 'projected': 0.004651162790697674,\n",
       " 'approached': 0.004651162790697674,\n",
       " 'instability': 0.004651162790697674}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the next word\n",
    "dict(model[\"the\",\"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c1666b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today the Higher Labour Tribunal in Brasilia is due to the remaining shares of the House of Commons today he said .\n"
     ]
    }
   ],
   "source": [
    "# code courtesy of https://nlpforhackers.io/language-models/\n",
    "\n",
    "import random\n",
    "\n",
    "# starting words\n",
    "text = [\"today\", \"the\"]\n",
    "sentence_finished = False\n",
    " \n",
    "while not sentence_finished:\n",
    "  # select a random probability threshold  \n",
    "  r = random.random()\n",
    "  accumulator = .0\n",
    "\n",
    "  for word in model[tuple(text[-2:])].keys():\n",
    "      accumulator += model[tuple(text[-2:])][word]\n",
    "      # select words that are above the probability threshold\n",
    "      if accumulator >= r:\n",
    "          text.append(word)\n",
    "          break\n",
    "\n",
    "  if text[-2:] == [None, None]:\n",
    "      sentence_finished = True\n",
    " \n",
    "print (' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73d8c6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8dbbd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp38-cp38-win_amd64.whl (438.0 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\downloads\\anaconda\\anaconda\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Collecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.44.0-cp38-cp38-win_amd64.whl (3.4 MB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-13.0.0-py2.py3-none-win_amd64.whl (13.9 MB)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in d:\\downloads\\anaconda\\anaconda\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.24.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.19.4-cp38-cp38-win_amd64.whl (895 kB)\n",
      "Requirement already satisfied: setuptools in d:\\downloads\\anaconda\\anaconda\\lib\\site-packages (from tensorflow) (52.0.0.post20210125)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\downloads\\anaconda\\anaconda\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in d:\\downloads\\anaconda\\anaconda\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\downloads\\anaconda\\anaconda\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in d:\\downloads\\anaconda\\anaconda\\lib\\site-packages (from tensorflow) (1.20.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\downloads\\anaconda\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\downloads\\anaconda\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.25.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.2-py2.py3-none-any.whl (156 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in d:\\downloads\\anaconda\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\downloads\\anaconda\\anaconda\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.4.1)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\downloads\\anaconda\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\downloads\\anaconda\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in d:\\downloads\\anaconda\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (4.0.0)\n",
      "\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\downloads\\anaconda\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2020.12.5)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=ac3590550d3b5132ff87a8e53fe6235a91b087512d4237853ccfcbfeebbe177a\n",
      "  Stored in directory: c:\\users\\abishek\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 3.10.0\n",
      "    Uninstalling importlib-metadata-3.10.0:\n",
      "      Successfully uninstalled importlib-metadata-3.10.0\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.2 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 importlib-metadata-4.11.3 keras-preprocessing-1.1.2 libclang-13.0.0 markdown-3.3.6 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.24.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ad7f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, GRU, Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b055d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = \"\"\"The unanimous Declaration of the thirteen united States of America, When in the Course of human events, it becomes necessary for one people to dissolve the political bands which have connected them with another, and to assume among the powers of the earth, the separate and equal station to which the Laws of Nature and of Nature's God entitle them, a decent respect to the opinions of mankind requires that they should declare the causes which impel them to the separation.\n",
    "We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.--That to secure these rights, Governments are instituted among Men, deriving their just powers from the consent of the governed, --That whenever any Form of Government becomes destructive of these ends, it is the Right of the People to alter or to abolish it, and to institute new Government, laying its foundation on such principles and organizing its powers in such form, as to them shall seem most likely to effect their Safety and Happiness. Prudence, indeed, will dictate that Governments long established should not be changed for light and transient causes; and accordingly all experience hath shewn, that mankind are more disposed to suffer, while evils are sufferable, than to right themselves by abolishing the forms to which they are accustomed. But when a long train of abuses and usurpations, pursuing invariably the same Object evinces a design to reduce them under absolute Despotism, it is their right, it is their duty, to throw off such Government, and to provide new Guards for their future security.--Such has been the patient sufferance of these Colonies; and such is now the necessity which constrains them to alter their former Systems of Government. The history of the present King of Great Britain is a history of repeated injuries and usurpations, all having in direct object the establishment of an absolute Tyranny over these States. To prove this, let Facts be submitted to a candid world.\n",
    "He has refused his Assent to Laws, the most wholesome and necessary for the public good.\n",
    "He has forbidden his Governors to pass Laws of immediate and pressing importance, unless suspended in their operation till his Assent should be obtained; and when so suspended, he has utterly neglected to attend to them.\n",
    "He has refused to pass other Laws for the accommodation of large districts of people, unless those people would relinquish the right of Representation in the Legislature, a right inestimable to them and formidable to tyrants only.\n",
    "He has called together legislative bodies at places unusual, uncomfortable, and distant from the depository of their public Records, for the sole purpose of fatiguing them into compliance with his measures.\n",
    "He has dissolved Representative Houses repeatedly, for opposing with manly firmness his invasions on the rights of the people.\n",
    "He has refused for a long time, after such dissolutions, to cause others to be elected; whereby the Legislative powers, incapable of Annihilation, have returned to the People at large for their exercise; the State remaining in the mean time exposed to all the dangers of invasion from without, and convulsions within.\n",
    "He has endeavoured to prevent the population of these States; for that purpose obstructing the Laws for Naturalization of Foreigners; refusing to pass others to encourage their migrations hither, and raising the conditions of new Appropriations of Lands.\n",
    "He has obstructed the Administration of Justice, by refusing his Assent to Laws for establishing Judiciary powers.\n",
    "He has made Judges dependent on his Will alone, for the tenure of their offices, and the amount and payment of their salaries.\n",
    "He has erected a multitude of New Offices, and sent hither swarms of Officers to harrass our people, and eat out their substance.\n",
    "He has kept among us, in times of peace, Standing Armies without the Consent of our legislatures.\n",
    "He has affected to render the Military independent of and superior to the Civil power.\n",
    "He has combined with others to subject us to a jurisdiction foreign to our constitution, and unacknowledged by our laws; giving his Assent to their Acts of pretended Legislation:\n",
    "For Quartering large bodies of armed troops among us:\n",
    "For protecting them, by a mock Trial, from punishment for any Murders which they should commit on the Inhabitants of these States:\n",
    "For cutting off our Trade with all parts of the world:\n",
    "For imposing Taxes on us without our Consent:\n",
    "For depriving us in many cases, of the benefits of Trial by Jury:\n",
    "For transporting us beyond Seas to be tried for pretended offences\n",
    "For abolishing the free System of English Laws in a neighbouring Province, establishing therein an Arbitrary government, and enlarging its Boundaries so as to render it at once an example and fit instrument for introducing the same absolute rule into these Colonies:\n",
    "For taking away our Charters, abolishing our most valuable Laws, and altering fundamentally the Forms of our Governments:\n",
    "For suspending our own Legislatures, and declaring themselves invested with power to legislate for us in all cases whatsoever.\n",
    "He has abdicated Government here, by declaring us out of his Protection and waging War against us.\n",
    "He has plundered our seas, ravaged our Coasts, burnt our towns, and destroyed the lives of our people.\n",
    "He is at this time transporting large Armies of foreign Mercenaries to compleat the works of death, desolation and tyranny, already begun with circumstances of Cruelty & perfidy scarcely paralleled in the most barbarous ages, and totally unworthy the Head of a civilized nation.\n",
    "He has constrained our fellow Citizens taken Captive on the high Seas to bear Arms against their Country, to become the executioners of their friends and Brethren, or to fall themselves by their Hands.\n",
    "He has excited domestic insurrections amongst us, and has endeavoured to bring on the inhabitants of our frontiers, the merciless Indian Savages, whose known rule of warfare, is an undistinguished destruction of all ages, sexes and conditions.\n",
    "In every stage of these Oppressions We have Petitioned for Redress in the most humble terms: Our repeated Petitions have been answered only by repeated injury. A Prince whose character is thus marked by every act which may define a Tyrant, is unfit to be the ruler of a free people.\n",
    "Nor have We been wanting in attentions to our Brittish brethren. We have warned them from time to time of attempts by their legislature to extend an unwarrantable jurisdiction over us. We have reminded them of the circumstances of our emigration and settlement here. We have appealed to their native justice and magnanimity, and we have conjured them by the ties of our common kindred to disavow these usurpations, which, would inevitably interrupt our connections and correspondence. They too have been deaf to the voice of justice and of consanguinity. We must, therefore, acquiesce in the necessity, which denounces our Separation, and hold them, as we hold the rest of mankind, Enemies in War, in Peace Friends.\n",
    "We, therefore, the Representatives of the united States of America, in General Congress, Assembled, appealing to the Supreme Judge of the world for the rectitude of our intentions, do, in the Name, and by Authority of the good People of these Colonies, solemnly publish and declare, That these United Colonies are, and of Right ought to be Free and Independent States; that they are Absolved from all Allegiance to the British Crown, and that all political connection between them and the State of Great Britain, is and ought to be totally dissolved; and that as Free and Independent States, they have full Power to levy War, conclude Peace, contract Alliances, establish Commerce, and to do all other Acts and Things which Independent States may of right do. And for the support of this Declaration, with a firm reliance on the protection of divine Providence, we mutually pledge to each other our Lives, our Fortunes and our sacred Honor.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "574985ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_cleaner(text):\n",
    "    # lower case text\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    # remove punctuations\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    long_words=[]\n",
    "    # remove short word\n",
    "    for i in newString.split():\n",
    "        if len(i)>=3:                  \n",
    "            long_words.append(i)\n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "# preprocess the text\n",
    "data_new = text_cleaner(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f86e1f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 7052\n"
     ]
    }
   ],
   "source": [
    "def create_seq(text):\n",
    "    length = 30\n",
    "    sequences = list()\n",
    "    for i in range(length, len(text)):\n",
    "        # select sequence of tokens\n",
    "        seq = text[i-length:i+1]\n",
    "        # store\n",
    "        sequences.append(seq)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    return sequences\n",
    "\n",
    "# create sequences   \n",
    "sequences = create_seq(data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20a2ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a character mapping index\n",
    "chars = sorted(list(set(data_new)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "def encode_seq(seq):\n",
    "    sequences = list()\n",
    "    for line in seq:\n",
    "        # integer encode line\n",
    "        encoded_seq = [mapping[char] for char in line]\n",
    "        # store\n",
    "        sequences.append(encoded_seq)\n",
    "    return sequences\n",
    "\n",
    "# encode the sequences\n",
    "sequences = encode_seq(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04c754c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (6346, 30) Val shape: (706, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# vocabulary size\n",
    "vocab = len(mapping)\n",
    "sequences = np.array(sequences)\n",
    "# create X and y\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "# one hot encode y\n",
    "y = to_categorical(y, num_classes=vocab)\n",
    "# create train and validation sets\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c72192e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 30, 50)            1350      \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 150)               90900     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 27)                4077      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 96,327\n",
      "Trainable params: 96,327\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "199/199 - 9s - loss: 2.7263 - acc: 0.2124 - val_loss: 2.3985 - val_acc: 0.3074 - 9s/epoch - 47ms/step\n",
      "Epoch 2/100\n",
      "199/199 - 6s - loss: 2.2982 - acc: 0.3188 - val_loss: 2.2418 - val_acc: 0.3343 - 6s/epoch - 33ms/step\n",
      "Epoch 3/100\n",
      "199/199 - 7s - loss: 2.1716 - acc: 0.3555 - val_loss: 2.1556 - val_acc: 0.3754 - 7s/epoch - 34ms/step\n",
      "Epoch 4/100\n",
      "199/199 - 7s - loss: 2.0744 - acc: 0.3818 - val_loss: 2.0813 - val_acc: 0.4065 - 7s/epoch - 34ms/step\n",
      "Epoch 5/100\n",
      "199/199 - 7s - loss: 1.9817 - acc: 0.4092 - val_loss: 2.0443 - val_acc: 0.4221 - 7s/epoch - 35ms/step\n",
      "Epoch 6/100\n",
      "199/199 - 7s - loss: 1.8956 - acc: 0.4286 - val_loss: 1.9924 - val_acc: 0.4433 - 7s/epoch - 36ms/step\n",
      "Epoch 7/100\n",
      "199/199 - 7s - loss: 1.8257 - acc: 0.4496 - val_loss: 1.9395 - val_acc: 0.4320 - 7s/epoch - 36ms/step\n",
      "Epoch 8/100\n",
      "199/199 - 7s - loss: 1.7450 - acc: 0.4704 - val_loss: 1.9064 - val_acc: 0.4603 - 7s/epoch - 37ms/step\n",
      "Epoch 9/100\n",
      "199/199 - 8s - loss: 1.6688 - acc: 0.4962 - val_loss: 1.9036 - val_acc: 0.4745 - 8s/epoch - 42ms/step\n",
      "Epoch 10/100\n",
      "199/199 - 8s - loss: 1.6012 - acc: 0.5170 - val_loss: 1.8830 - val_acc: 0.4618 - 8s/epoch - 38ms/step\n",
      "Epoch 11/100\n",
      "199/199 - 9s - loss: 1.5317 - acc: 0.5383 - val_loss: 1.8767 - val_acc: 0.4816 - 9s/epoch - 44ms/step\n",
      "Epoch 12/100\n",
      "199/199 - 10s - loss: 1.4627 - acc: 0.5567 - val_loss: 1.8740 - val_acc: 0.4703 - 10s/epoch - 48ms/step\n",
      "Epoch 13/100\n",
      "199/199 - 9s - loss: 1.3978 - acc: 0.5730 - val_loss: 1.8680 - val_acc: 0.4788 - 9s/epoch - 43ms/step\n",
      "Epoch 14/100\n",
      "199/199 - 8s - loss: 1.3388 - acc: 0.5903 - val_loss: 1.8762 - val_acc: 0.4816 - 8s/epoch - 42ms/step\n",
      "Epoch 15/100\n",
      "199/199 - 8s - loss: 1.2861 - acc: 0.6061 - val_loss: 1.8776 - val_acc: 0.4773 - 8s/epoch - 43ms/step\n",
      "Epoch 16/100\n",
      "199/199 - 9s - loss: 1.2284 - acc: 0.6250 - val_loss: 1.8668 - val_acc: 0.4929 - 9s/epoch - 43ms/step\n",
      "Epoch 17/100\n",
      "199/199 - 9s - loss: 1.1783 - acc: 0.6343 - val_loss: 1.8908 - val_acc: 0.4887 - 9s/epoch - 43ms/step\n",
      "Epoch 18/100\n",
      "199/199 - 8s - loss: 1.1279 - acc: 0.6551 - val_loss: 1.9052 - val_acc: 0.4816 - 8s/epoch - 41ms/step\n",
      "Epoch 19/100\n",
      "199/199 - 8s - loss: 1.0852 - acc: 0.6618 - val_loss: 1.9110 - val_acc: 0.4816 - 8s/epoch - 41ms/step\n",
      "Epoch 20/100\n",
      "199/199 - 8s - loss: 1.0352 - acc: 0.6804 - val_loss: 1.9317 - val_acc: 0.4858 - 8s/epoch - 43ms/step\n",
      "Epoch 21/100\n",
      "199/199 - 8s - loss: 0.9981 - acc: 0.6875 - val_loss: 1.9508 - val_acc: 0.4915 - 8s/epoch - 41ms/step\n",
      "Epoch 22/100\n",
      "199/199 - 8s - loss: 0.9513 - acc: 0.7041 - val_loss: 1.9700 - val_acc: 0.4873 - 8s/epoch - 42ms/step\n",
      "Epoch 23/100\n",
      "199/199 - 8s - loss: 0.9196 - acc: 0.7110 - val_loss: 2.0044 - val_acc: 0.4830 - 8s/epoch - 42ms/step\n",
      "Epoch 24/100\n",
      "199/199 - 8s - loss: 0.8945 - acc: 0.7189 - val_loss: 2.0013 - val_acc: 0.4858 - 8s/epoch - 41ms/step\n",
      "Epoch 25/100\n",
      "199/199 - 9s - loss: 0.8530 - acc: 0.7247 - val_loss: 2.0472 - val_acc: 0.4816 - 9s/epoch - 43ms/step\n",
      "Epoch 26/100\n",
      "199/199 - 8s - loss: 0.8244 - acc: 0.7413 - val_loss: 2.0698 - val_acc: 0.4788 - 8s/epoch - 39ms/step\n",
      "Epoch 27/100\n",
      "199/199 - 8s - loss: 0.7896 - acc: 0.7528 - val_loss: 2.0996 - val_acc: 0.4759 - 8s/epoch - 38ms/step\n",
      "Epoch 28/100\n",
      "199/199 - 7s - loss: 0.7696 - acc: 0.7616 - val_loss: 2.1280 - val_acc: 0.4788 - 7s/epoch - 37ms/step\n",
      "Epoch 29/100\n",
      "199/199 - 8s - loss: 0.7384 - acc: 0.7736 - val_loss: 2.1570 - val_acc: 0.4830 - 8s/epoch - 38ms/step\n",
      "Epoch 30/100\n",
      "199/199 - 7s - loss: 0.7193 - acc: 0.7714 - val_loss: 2.1779 - val_acc: 0.4773 - 7s/epoch - 37ms/step\n",
      "Epoch 31/100\n",
      "199/199 - 7s - loss: 0.7024 - acc: 0.7806 - val_loss: 2.2172 - val_acc: 0.4773 - 7s/epoch - 37ms/step\n",
      "Epoch 32/100\n",
      "199/199 - 7s - loss: 0.6787 - acc: 0.7843 - val_loss: 2.2169 - val_acc: 0.4858 - 7s/epoch - 37ms/step\n",
      "Epoch 33/100\n",
      "199/199 - 7s - loss: 0.6543 - acc: 0.7909 - val_loss: 2.2397 - val_acc: 0.4816 - 7s/epoch - 37ms/step\n",
      "Epoch 34/100\n",
      "199/199 - 8s - loss: 0.6315 - acc: 0.7978 - val_loss: 2.2686 - val_acc: 0.4873 - 8s/epoch - 39ms/step\n",
      "Epoch 35/100\n",
      "199/199 - 8s - loss: 0.6152 - acc: 0.7992 - val_loss: 2.3007 - val_acc: 0.4816 - 8s/epoch - 41ms/step\n",
      "Epoch 36/100\n",
      "199/199 - 9s - loss: 0.5996 - acc: 0.8144 - val_loss: 2.3185 - val_acc: 0.4830 - 9s/epoch - 44ms/step\n",
      "Epoch 37/100\n",
      "199/199 - 9s - loss: 0.5776 - acc: 0.8150 - val_loss: 2.3520 - val_acc: 0.4674 - 9s/epoch - 45ms/step\n",
      "Epoch 38/100\n",
      "199/199 - 8s - loss: 0.5744 - acc: 0.8180 - val_loss: 2.3446 - val_acc: 0.4745 - 8s/epoch - 40ms/step\n",
      "Epoch 39/100\n",
      "199/199 - 8s - loss: 0.5705 - acc: 0.8158 - val_loss: 2.3743 - val_acc: 0.4759 - 8s/epoch - 40ms/step\n",
      "Epoch 40/100\n",
      "199/199 - 8s - loss: 0.5447 - acc: 0.8256 - val_loss: 2.4148 - val_acc: 0.4802 - 8s/epoch - 41ms/step\n",
      "Epoch 41/100\n",
      "199/199 - 8s - loss: 0.5227 - acc: 0.8304 - val_loss: 2.5082 - val_acc: 0.4632 - 8s/epoch - 40ms/step\n",
      "Epoch 42/100\n",
      "199/199 - 8s - loss: 0.5245 - acc: 0.8356 - val_loss: 2.5227 - val_acc: 0.4674 - 8s/epoch - 40ms/step\n",
      "Epoch 43/100\n",
      "199/199 - 8s - loss: 0.5034 - acc: 0.8360 - val_loss: 2.5001 - val_acc: 0.4717 - 8s/epoch - 41ms/step\n",
      "Epoch 44/100\n",
      "199/199 - 8s - loss: 0.5048 - acc: 0.8394 - val_loss: 2.5255 - val_acc: 0.4745 - 8s/epoch - 40ms/step\n",
      "Epoch 45/100\n",
      "199/199 - 7s - loss: 0.4864 - acc: 0.8413 - val_loss: 2.5298 - val_acc: 0.4830 - 7s/epoch - 35ms/step\n",
      "Epoch 46/100\n",
      "199/199 - 7s - loss: 0.4636 - acc: 0.8498 - val_loss: 2.5931 - val_acc: 0.4773 - 7s/epoch - 37ms/step\n",
      "Epoch 47/100\n",
      "199/199 - 8s - loss: 0.4675 - acc: 0.8497 - val_loss: 2.6112 - val_acc: 0.4674 - 8s/epoch - 38ms/step\n",
      "Epoch 48/100\n",
      "199/199 - 7s - loss: 0.4521 - acc: 0.8571 - val_loss: 2.6324 - val_acc: 0.4646 - 7s/epoch - 37ms/step\n",
      "Epoch 49/100\n",
      "199/199 - 7s - loss: 0.4426 - acc: 0.8585 - val_loss: 2.6993 - val_acc: 0.4561 - 7s/epoch - 35ms/step\n",
      "Epoch 50/100\n",
      "199/199 - 7s - loss: 0.4356 - acc: 0.8602 - val_loss: 2.6617 - val_acc: 0.4759 - 7s/epoch - 35ms/step\n",
      "Epoch 51/100\n",
      "199/199 - 7s - loss: 0.4377 - acc: 0.8594 - val_loss: 2.6688 - val_acc: 0.4759 - 7s/epoch - 36ms/step\n",
      "Epoch 52/100\n",
      "199/199 - 7s - loss: 0.4253 - acc: 0.8650 - val_loss: 2.7217 - val_acc: 0.4703 - 7s/epoch - 36ms/step\n",
      "Epoch 53/100\n",
      "199/199 - 7s - loss: 0.4154 - acc: 0.8697 - val_loss: 2.7443 - val_acc: 0.4674 - 7s/epoch - 36ms/step\n",
      "Epoch 54/100\n",
      "199/199 - 7s - loss: 0.4072 - acc: 0.8645 - val_loss: 2.7419 - val_acc: 0.4688 - 7s/epoch - 35ms/step\n",
      "Epoch 55/100\n",
      "199/199 - 7s - loss: 0.4066 - acc: 0.8743 - val_loss: 2.7808 - val_acc: 0.4688 - 7s/epoch - 36ms/step\n",
      "Epoch 56/100\n",
      "199/199 - 7s - loss: 0.3874 - acc: 0.8780 - val_loss: 2.7855 - val_acc: 0.4660 - 7s/epoch - 35ms/step\n",
      "Epoch 57/100\n",
      "199/199 - 7s - loss: 0.3945 - acc: 0.8720 - val_loss: 2.8201 - val_acc: 0.4589 - 7s/epoch - 35ms/step\n",
      "Epoch 58/100\n",
      "199/199 - 7s - loss: 0.4029 - acc: 0.8719 - val_loss: 2.8158 - val_acc: 0.4688 - 7s/epoch - 35ms/step\n",
      "Epoch 59/100\n",
      "199/199 - 7s - loss: 0.3824 - acc: 0.8807 - val_loss: 2.8491 - val_acc: 0.4717 - 7s/epoch - 35ms/step\n",
      "Epoch 60/100\n",
      "199/199 - 7s - loss: 0.3938 - acc: 0.8689 - val_loss: 2.8707 - val_acc: 0.4589 - 7s/epoch - 35ms/step\n",
      "Epoch 61/100\n",
      "199/199 - 7s - loss: 0.3715 - acc: 0.8793 - val_loss: 2.9303 - val_acc: 0.4703 - 7s/epoch - 35ms/step\n",
      "Epoch 62/100\n",
      "199/199 - 7s - loss: 0.3712 - acc: 0.8795 - val_loss: 2.9235 - val_acc: 0.4632 - 7s/epoch - 35ms/step\n",
      "Epoch 63/100\n",
      "199/199 - 7s - loss: 0.3598 - acc: 0.8880 - val_loss: 2.9406 - val_acc: 0.4731 - 7s/epoch - 35ms/step\n",
      "Epoch 64/100\n",
      "199/199 - 7s - loss: 0.3577 - acc: 0.8851 - val_loss: 2.9371 - val_acc: 0.4603 - 7s/epoch - 35ms/step\n",
      "Epoch 65/100\n",
      "199/199 - 7s - loss: 0.3523 - acc: 0.8842 - val_loss: 2.9635 - val_acc: 0.4731 - 7s/epoch - 36ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100\n",
      "199/199 - 7s - loss: 0.3491 - acc: 0.8861 - val_loss: 2.9654 - val_acc: 0.4618 - 7s/epoch - 38ms/step\n",
      "Epoch 67/100\n",
      "199/199 - 7s - loss: 0.3410 - acc: 0.8913 - val_loss: 2.9925 - val_acc: 0.4688 - 7s/epoch - 37ms/step\n",
      "Epoch 68/100\n",
      "199/199 - 8s - loss: 0.3419 - acc: 0.8895 - val_loss: 3.0005 - val_acc: 0.4717 - 8s/epoch - 40ms/step\n",
      "Epoch 69/100\n",
      "199/199 - 7s - loss: 0.3573 - acc: 0.8834 - val_loss: 2.9960 - val_acc: 0.4816 - 7s/epoch - 37ms/step\n",
      "Epoch 70/100\n",
      "199/199 - 8s - loss: 0.3371 - acc: 0.8900 - val_loss: 3.0618 - val_acc: 0.4589 - 8s/epoch - 40ms/step\n",
      "Epoch 71/100\n",
      "199/199 - 8s - loss: 0.3453 - acc: 0.8889 - val_loss: 3.0275 - val_acc: 0.4717 - 8s/epoch - 38ms/step\n",
      "Epoch 72/100\n",
      "199/199 - 7s - loss: 0.3280 - acc: 0.8933 - val_loss: 3.0426 - val_acc: 0.4745 - 7s/epoch - 35ms/step\n",
      "Epoch 73/100\n",
      "199/199 - 7s - loss: 0.3170 - acc: 0.9023 - val_loss: 3.0338 - val_acc: 0.4717 - 7s/epoch - 35ms/step\n",
      "Epoch 74/100\n",
      "199/199 - 8s - loss: 0.3270 - acc: 0.8962 - val_loss: 3.0975 - val_acc: 0.4731 - 8s/epoch - 39ms/step\n",
      "Epoch 75/100\n",
      "199/199 - 7s - loss: 0.3257 - acc: 0.8935 - val_loss: 3.1274 - val_acc: 0.4717 - 7s/epoch - 36ms/step\n",
      "Epoch 76/100\n",
      "199/199 - 7s - loss: 0.3317 - acc: 0.8921 - val_loss: 3.1494 - val_acc: 0.4646 - 7s/epoch - 35ms/step\n",
      "Epoch 77/100\n",
      "199/199 - 7s - loss: 0.3267 - acc: 0.8902 - val_loss: 3.1226 - val_acc: 0.4745 - 7s/epoch - 36ms/step\n",
      "Epoch 78/100\n",
      "199/199 - 7s - loss: 0.3137 - acc: 0.8925 - val_loss: 3.1414 - val_acc: 0.4745 - 7s/epoch - 35ms/step\n",
      "Epoch 79/100\n",
      "199/199 - 7s - loss: 0.3133 - acc: 0.8954 - val_loss: 3.2017 - val_acc: 0.4688 - 7s/epoch - 35ms/step\n",
      "Epoch 80/100\n",
      "199/199 - 7s - loss: 0.3133 - acc: 0.8988 - val_loss: 3.2064 - val_acc: 0.4788 - 7s/epoch - 35ms/step\n",
      "Epoch 81/100\n",
      "199/199 - 7s - loss: 0.3020 - acc: 0.9010 - val_loss: 3.2213 - val_acc: 0.4533 - 7s/epoch - 35ms/step\n",
      "Epoch 82/100\n",
      "199/199 - 7s - loss: 0.3229 - acc: 0.8906 - val_loss: 3.2185 - val_acc: 0.4731 - 7s/epoch - 36ms/step\n",
      "Epoch 83/100\n",
      "199/199 - 7s - loss: 0.3006 - acc: 0.8974 - val_loss: 3.2172 - val_acc: 0.4674 - 7s/epoch - 36ms/step\n",
      "Epoch 84/100\n",
      "199/199 - 7s - loss: 0.2982 - acc: 0.9012 - val_loss: 3.2605 - val_acc: 0.4717 - 7s/epoch - 35ms/step\n",
      "Epoch 85/100\n",
      "199/199 - 7s - loss: 0.3068 - acc: 0.9009 - val_loss: 3.2488 - val_acc: 0.4646 - 7s/epoch - 36ms/step\n",
      "Epoch 86/100\n",
      "199/199 - 7s - loss: 0.2870 - acc: 0.9089 - val_loss: 3.2461 - val_acc: 0.4745 - 7s/epoch - 36ms/step\n",
      "Epoch 87/100\n",
      "199/199 - 7s - loss: 0.2912 - acc: 0.9007 - val_loss: 3.2806 - val_acc: 0.4703 - 7s/epoch - 35ms/step\n",
      "Epoch 88/100\n",
      "199/199 - 7s - loss: 0.3009 - acc: 0.8987 - val_loss: 3.2843 - val_acc: 0.4844 - 7s/epoch - 35ms/step\n",
      "Epoch 89/100\n",
      "199/199 - 7s - loss: 0.2899 - acc: 0.9056 - val_loss: 3.3389 - val_acc: 0.4618 - 7s/epoch - 35ms/step\n",
      "Epoch 90/100\n",
      "199/199 - 7s - loss: 0.2791 - acc: 0.9072 - val_loss: 3.3437 - val_acc: 0.4646 - 7s/epoch - 35ms/step\n",
      "Epoch 91/100\n",
      "199/199 - 7s - loss: 0.2878 - acc: 0.9018 - val_loss: 3.3407 - val_acc: 0.4745 - 7s/epoch - 35ms/step\n",
      "Epoch 92/100\n",
      "199/199 - 7s - loss: 0.2923 - acc: 0.9042 - val_loss: 3.3382 - val_acc: 0.4745 - 7s/epoch - 36ms/step\n",
      "Epoch 93/100\n",
      "199/199 - 7s - loss: 0.2857 - acc: 0.9012 - val_loss: 3.3725 - val_acc: 0.4660 - 7s/epoch - 36ms/step\n",
      "Epoch 94/100\n",
      "199/199 - 7s - loss: 0.2828 - acc: 0.9048 - val_loss: 3.3749 - val_acc: 0.4603 - 7s/epoch - 36ms/step\n",
      "Epoch 95/100\n",
      "199/199 - 7s - loss: 0.2770 - acc: 0.9058 - val_loss: 3.3815 - val_acc: 0.4632 - 7s/epoch - 36ms/step\n",
      "Epoch 96/100\n",
      "199/199 - 7s - loss: 0.2778 - acc: 0.9075 - val_loss: 3.3343 - val_acc: 0.4703 - 7s/epoch - 35ms/step\n",
      "Epoch 97/100\n",
      "199/199 - 7s - loss: 0.2846 - acc: 0.9032 - val_loss: 3.4202 - val_acc: 0.4603 - 7s/epoch - 37ms/step\n",
      "Epoch 98/100\n",
      "199/199 - 7s - loss: 0.2874 - acc: 0.9043 - val_loss: 3.4089 - val_acc: 0.4660 - 7s/epoch - 36ms/step\n",
      "Epoch 99/100\n",
      "199/199 - 7s - loss: 0.2788 - acc: 0.9045 - val_loss: 3.4102 - val_acc: 0.4561 - 7s/epoch - 36ms/step\n",
      "Epoch 100/100\n",
      "199/199 - 7s - loss: 0.2711 - acc: 0.9086 - val_loss: 3.4314 - val_acc: 0.4603 - 7s/epoch - 37ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1db994ae100>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab, 50, input_length=30, trainable=True))\n",
    "model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\n",
    "model.add(Dense(vocab, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "# fit the model\n",
    "model.fit(X_tr, y_tr, epochs=100, verbose=2, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1609d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence of characters with a language model\n",
    "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of characters\n",
    "\tfor _ in range(n_chars):\n",
    "\t\t# encode the characters as integers\n",
    "\t\tencoded = [mapping[char] for char in in_text]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict character\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# reverse map integer to character\n",
    "\t\tout_char = ''\n",
    "\t\tfor char, index in mapping.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_char = char\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += char\n",
    "\treturn in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cb2f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
